{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren der entsprechenden Packages für das Ausführen des Skripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy.linalg import svd\n",
    "from scipy.optimize import least_squares, differential_evolution\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import time\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen zur Umwandlung der Encoderwerte in Winkel (rad):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelenk0_TF(encoder_raw_position):\n",
    "        \"\"\"\n",
    "        Transformation logic for Gelenk 0.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"a\": 2450.0517124242577,\n",
    "            \"b\": 7867.202696571338,\n",
    "            \"c1\": 5414.61226810207,\n",
    "            \"gamma0\": -1.4348954481458154,\n",
    "            \"inverted\": True,\n",
    "                    }\n",
    "\n",
    "        inverted = params['inverted']\n",
    "        a, b, c1, gamma0 = params['a'], params['b'], params['c1'], params['gamma0']\n",
    "\n",
    "        if (inverted == True):\n",
    "            return (-1) * (np.arccos((a ** 2 + b ** 2 - (c1 + encoder_raw_position) ** 2) / (2 * a * b)) + gamma0)\n",
    "        else:\n",
    "            return np.arccos((a ** 2 + b ** 2 - (c1 + encoder_raw_position) ** 2) / (2 * a * b)) + gamma0\n",
    "        \n",
    "def gelenk1_TF(encoder_raw_position):\n",
    "        \"\"\"\n",
    "        Transformation logic for Gelenk 1.\n",
    "        \"\"\"\n",
    "        params={\n",
    "        \"offset\": 33125,\n",
    "        \"resolution_bit\": 18,\n",
    "        \"inverted\": False}\n",
    "\n",
    "        inverted = params['inverted']\n",
    "        offset = params['offset']\n",
    "        resolution_bit = params['resolution_bit']\n",
    "        gain = pow(2, -resolution_bit) * 2 * np.pi\n",
    "\n",
    "        adjusted_position = (encoder_raw_position - offset) % (2 ** resolution_bit)\n",
    "        transformed_points_1 = []\n",
    "        for position in adjusted_position:\n",
    "            if position > (2 ** resolution_bit) / 2:\n",
    "                value = position - (2 ** resolution_bit)\n",
    "                transformed_points_1.append(value)\n",
    "            else:\n",
    "                 transformed_points_1.append(position)\n",
    "\n",
    "        transformed_points_1 = pd.DataFrame(transformed_points_1, columns=['encoder_1'],index=encoder_raw_position.index)\n",
    "        \n",
    "        if (inverted == False):\n",
    "            return transformed_points_1 * gain\n",
    "        else:\n",
    "            return (-1) * (transformed_points_1 * gain)\n",
    "\n",
    "def gelenk2_TF(encoder_raw_position):\n",
    "        \"\"\"\n",
    "        Transformation logic for Gelenk 2.\n",
    "        \"\"\"\n",
    "\n",
    "        params ={\n",
    "        \"offset\": 669,\n",
    "        \"gain\": 0.0001, \n",
    "        \"inverted\": False}\n",
    "\n",
    "        inverted = params['inverted']\n",
    "        offset = params['offset']\n",
    "        gain = params['gain']\n",
    "    \n",
    "        if (inverted == False):\n",
    "            return (encoder_raw_position - offset) * gain\n",
    "        else:\n",
    "            return (-1) * ((encoder_raw_position - offset) * gain)\n",
    "\n",
    "def gelenk3_TF(encoder_raw_position):\n",
    "        \"\"\"\n",
    "        Transformation logic for Gelenk 3.\n",
    "        \"\"\"\n",
    "\n",
    "        params={\n",
    "        \"offset\": 150339,\n",
    "        \"resolution_bit\": 18,\n",
    "        \"inverted\": False}\n",
    "\n",
    "        inverted = params['inverted']\n",
    "        offset = params['offset']\n",
    "        resolution_bit = params['resolution_bit']\n",
    "        gain = pow(2, -resolution_bit) * 2 * np.pi\n",
    "\n",
    "        adjusted_position = (encoder_raw_position - offset) % (2 ** resolution_bit)\n",
    "        transformed_points_3 = []\n",
    "        for position in adjusted_position:\n",
    "            if position > (2 ** resolution_bit) / 2:\n",
    "                value = position - (2 ** resolution_bit)\n",
    "                transformed_points_3.append(value)\n",
    "            else:\n",
    "                 transformed_points_3.append(position)\n",
    "\n",
    "        transformed_points_3 = pd.DataFrame(transformed_points_3, columns=['encoder_3'],index=encoder_raw_position.index)\n",
    "        \n",
    "        if (inverted == False):\n",
    "            return transformed_points_3 * gain\n",
    "        else:\n",
    "            return (-1) * (transformed_points_3 * gain)\n",
    "        \n",
    "\n",
    "def gelenk4_TF(encoder_raw_position):\n",
    "        \"\"\"\n",
    "        Transformation logic for Gelenk 4.\n",
    "        \"\"\"\n",
    "\n",
    "        params={\n",
    "        \"offset\": 186926,\n",
    "        \"resolution_bit\": 18,\n",
    "        \"inverted\": True}\n",
    "\n",
    "        inverted = params['inverted']\n",
    "        offset = params['offset']\n",
    "        resolution_bit = params['resolution_bit']\n",
    "        gain = pow(2, -resolution_bit) * 2 * np.pi\n",
    "\n",
    "        adjusted_position = (encoder_raw_position - offset) % (2 ** resolution_bit)\n",
    "        transformed_points_4 = []\n",
    "        for position in adjusted_position:\n",
    "            if position > (2 ** resolution_bit) / 2:\n",
    "                value = position - (2 ** resolution_bit)\n",
    "                transformed_points_4.append(value)\n",
    "            else:\n",
    "                 transformed_points_4.append(position)\n",
    "\n",
    "        transformed_points_4 = pd.DataFrame(transformed_points_4, columns=['encoder_4'],index=encoder_raw_position.index)\n",
    "        \n",
    "        if (inverted == False):\n",
    "            return transformed_points_4 * gain\n",
    "        else:\n",
    "            return (-1) * (transformed_points_4 * gain)\n",
    "\n",
    "def gelenk5_TF(encoder_raw_position):\n",
    "        \"\"\"\n",
    "        Transformation logic for Gelenk 5.\n",
    "        \"\"\"\n",
    "\n",
    "        params={\n",
    "        \"offset\": 33459832,\n",
    "        \"gain\": 0.0000343397756528412,\n",
    "        \"inverted\": False}\n",
    "\n",
    "        inverted = params['inverted']\n",
    "        offset = params['offset']\n",
    "        gain = params['gain']\n",
    "\n",
    "        if (inverted == False):\n",
    "            return (encoder_raw_position - offset) * gain\n",
    "        else:\n",
    "            return (-1) * ((encoder_raw_position - offset) * gain)\n",
    "\n",
    "def gelenk6_TF(encoder_raw_position):\n",
    "        \"\"\"\n",
    "        Transformation logic for Gelenk 6.\n",
    "        \"\"\"\n",
    "\n",
    "        params={\n",
    "        \"offset\": 1244,\n",
    "        \"resolution_bit\": 13,\n",
    "        \"inverted\": True}\n",
    "\n",
    "        inverted = params['inverted']\n",
    "        offset = params['offset']\n",
    "        resolution_bit = params['resolution_bit']\n",
    "        gain = pow(2, -resolution_bit) * 2 * np.pi\n",
    "\n",
    "        adjusted_position = (encoder_raw_position - offset) % (2 ** resolution_bit)\n",
    "        transformed_points_6 = []\n",
    "        for position in adjusted_position:\n",
    "            if position > (2 ** resolution_bit) / 2:\n",
    "                value = position - (2 ** resolution_bit)\n",
    "                transformed_points_6.append(value)\n",
    "            else:\n",
    "                 transformed_points_6.append(position)\n",
    "\n",
    "        transformed_points_6 = pd.DataFrame(transformed_points_6, columns=['encoder_6'],index=encoder_raw_position.index)\n",
    "        \n",
    "        if (inverted == False):\n",
    "            return transformed_points_6 * gain\n",
    "        else:\n",
    "            return (-1) * (transformed_points_6 * gain)\n",
    "\n",
    "def gelenk7_TF(encoder_raw_position):\n",
    "        \"\"\"\n",
    "        Transformation logic for Gelenk 7.\n",
    "        \"\"\"\n",
    "\n",
    "        params={\n",
    "        \"offset\": 7266,\n",
    "        \"resolution_bit\": 13,\n",
    "        \"inverted\": False}\n",
    "\n",
    "        inverted = params['inverted']\n",
    "        offset = params['offset']\n",
    "        resolution_bit = params['resolution_bit']\n",
    "        gain = pow(2, -resolution_bit) * 2 * np.pi\n",
    "\n",
    "        adjusted_position = (encoder_raw_position - offset) % (2 ** resolution_bit)\n",
    "        transformed_points_7 = []\n",
    "        for position in adjusted_position:\n",
    "            if position > (2 ** resolution_bit) / 2:\n",
    "                value = position - (2 ** resolution_bit)\n",
    "                transformed_points_7.append(value)\n",
    "            else:\n",
    "                 transformed_points_7.append(position)\n",
    "\n",
    "        transformed_points_7 = pd.DataFrame(transformed_points_7, columns=['encoder_3'],index=encoder_raw_position.index)\n",
    "        \n",
    "        if (inverted == False):\n",
    "            return transformed_points_7 * gain\n",
    "        else:\n",
    "            return (-1) * (transformed_points_7 * gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen zur Ermittlung der Transformation mittels optimiertem Einmessverfahren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch_algorithm(P, Q):\n",
    "    p_centroid = np.mean(P, axis=0)\n",
    "    q_centroid = np.mean(Q, axis=0)\n",
    "    P_centered = P - p_centroid # Werte werden ins Zentrum des Koordinatensystems verschoben\n",
    "    Q_centered = Q - q_centroid # Werte werden ins Zentrum des Koordinatensystems verschoben\n",
    "    H = P_centered.T @ Q_centered # Korrelationsmatrix\n",
    "    U, S, Vt = svd(H)\n",
    "    V = Vt.T\n",
    "    R = V @ U.T\n",
    "    if np.linalg.det(R) < 0:\n",
    "        V[:,-1] *= -1\n",
    "        R = V @ U.T\n",
    "    t = q_centroid - R @ p_centroid\n",
    "    return R, t\n",
    "\n",
    "def rotation_matrix_from_euler(alpha, beta, gamma):\n",
    "    # Rotationsmatrizen für Eulerwinkel (Z-Y-X)\n",
    "    Rz = np.array([[np.cos(alpha), -np.sin(alpha), 0],\n",
    "                   [np.sin(alpha),  np.cos(alpha), 0],\n",
    "                   [0,              0,             1]])\n",
    "    Ry = np.array([[ np.cos(beta), 0, np.sin(beta)],\n",
    "                   [0,             1, 0],\n",
    "                   [-np.sin(beta), 0, np.cos(beta)]])\n",
    "    Rx = np.array([[1,             0,              0],\n",
    "                   [0, np.cos(gamma), -np.sin(gamma)],\n",
    "                   [0, np.sin(gamma),  np.cos(gamma)]])\n",
    "    return Rz @ Ry @ Rx\n",
    "\n",
    "def residual(params, P, Q):\n",
    "    alpha, beta, gamma, tx, ty, tz = params\n",
    "    R_opt = rotation_matrix_from_euler(alpha, beta, gamma)\n",
    "    t_opt = np.array([tx, ty, tz])\n",
    "    P_transformed = (R_opt @ P.T).T + t_opt\n",
    "    diff = P_transformed - Q\n",
    "    return diff.flatten()\n",
    "\n",
    "\n",
    "# === 1. Globale Einstellungen für ALLE Plots ===\n",
    "rcParams.update({\n",
    "    # Schriftart und LaTeX\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Latin Modern Sans\"],\n",
    "    \"font.size\": 10,              # Standard-Schriftgröße für Paper\n",
    "    \"axes.labelsize\": 10,          # Achsentitelgröße\n",
    "    \"axes.titlesize\": 12,          # Plottitelgröße\n",
    "    \"legend.fontsize\": 9,          # Legendentext\n",
    "    \"xtick.labelsize\": 9,          # Tick-Label-Größe X\n",
    "    \"ytick.labelsize\": 9,          # Tick-Label-Größe Y\n",
    "\n",
    "    # Plotgröße und Auflösung\n",
    "    \"figure.figsize\": (6, 4),      # etwa 15x10 cm für LaTeX-Spaltenbreite\n",
    "    \"figure.dpi\": 300,\n",
    "\n",
    "    # Ränder\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \n",
    "    # Linien und Gitter\n",
    "    \"axes.grid\": True,             # Gitter standardmäßig an\n",
    "    \"grid.color\": \"gray\",\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \n",
    "    # Achsenlinien\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \n",
    "    # Ticks\n",
    "    \"xtick.direction\": \"in\",       # Ticks nach innen\n",
    "    \"ytick.direction\": \"in\",\n",
    "    \"xtick.major.size\": 5,         # Ticklänge\n",
    "    \"ytick.major.size\": 5,\n",
    "    \"xtick.minor.size\": 2.5,       # Kleine Ticks\n",
    "    \"ytick.minor.size\": 2.5,\n",
    "})\n",
    "\n",
    "\n",
    "# Seaborn-Style zusätzlich anpassen (optional für extra schönes Design)\n",
    "sns.set_theme(context='paper', style='white', font_scale=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laden und Formatieren der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data = pd.read_csv('../data/Modellentwicklung_Encoderwerte.csv', sep=';', index_col=0)\n",
    "raw_data = pd.read_table('../data/Modellentwicklung_Rohdaten_Totalstation.csv', delimiter=';',index_col=0)\n",
    "raw_data.columns = ['x_TS', 'y_TS', 'z_TS']\n",
    "\n",
    "# Messwerte für das optimierte Einmessverfahren filtern\n",
    "data_positioning = data[\n",
    "    (data['Norm'] < 20) & \n",
    "    (data['x_Encoder'] < 2000) & \n",
    "    (data['x_Encoder'] > -1200) & \n",
    "    (data['y_Encoder'] < 8000) & \n",
    "    (data['z_Encoder'] > -1000) & \n",
    "    (data['z_Encoder'] < 2200)\n",
    "]\n",
    "data_positioning.index = data_positioning.index\n",
    "raw_data_positioning = raw_data[raw_data.index.isin(data_positioning.index)]\n",
    "raw_data_positioning = raw_data_positioning.sort_index()\n",
    "data_positioning = data_positioning.sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ermittlung der Transformation zwischen Totalstation- und Maschinenkoordinatensystem (optimiertes Einmessverfahren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mittlerer Fehler der Norm, nach Transformation: 6.337410432977907\n",
      "Rotationsmatrix: [[-0.99954392  0.02321207  0.01931706]\n",
      " [-0.02282896 -0.99954285  0.01982235]\n",
      " [ 0.01976834  0.01937232  0.99961689]]\n",
      "Translationsvektor: [ 954.96817307 2009.34637261 -158.18073551]\n"
     ]
    }
   ],
   "source": [
    "# Messdaten für die Transformationsbestimmung vorbereiten\n",
    "P = raw_data_positioning[['x_TS','y_TS','z_TS']].values\n",
    "Q = data_positioning[['x_Encoder','y_Encoder','z_Encoder']].values/1000\n",
    "\n",
    "# Erster Schritt: Kabsch-Algorithmus zur Initialschätzung\n",
    "R_est, t_est = kabsch_algorithm(P, Q)\n",
    "\n",
    "# Initialschätzung für die Optimierung\n",
    "x0 = np.array([0.0, 0.0, 0.0, t_est[0], t_est[1], t_est[2]])\n",
    "\n",
    "# Robuste Optimierung mit Huber-Loss Funktion\n",
    "result = least_squares(residual, x0, args=(P, Q), loss='huber', f_scale=1.0)\n",
    "alpha_opt, beta_opt, gamma_opt, tx_opt, ty_opt, tz_opt = result.x\n",
    "residuals_after_kabsch_local = residual(result.x, P, Q)\n",
    "R_opt = rotation_matrix_from_euler(alpha_opt, beta_opt, gamma_opt)\n",
    "t_opt = np.array([tx_opt, ty_opt, tz_opt])\n",
    "\n",
    "# Berechnung des durchschnittlichen Fehler nach der Optimierung\n",
    "err_final = np.linalg.norm((R_opt @ P.T).T + t_opt - Q, axis=1)\n",
    "err_final_mean = np.mean(err_final)\n",
    "err_final_mean_squared = np.sqrt(np.mean((((R_opt @ P.T).T + t_opt) - Q)**2))\n",
    "print(\"Mittlerer Fehler der Norm, nach Transformation:\", err_final_mean*1000)\n",
    "print(\"Rotationsmatrix:\", R_opt)\n",
    "print(\"Translationsvektor:\", t_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umrechnung der Totalstation-Rohkoordinaten in Maschinenkoordinaten (Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation der Totalstation-Messwerte\n",
    "data = data.drop(columns=['x_TS', 'y_TS', 'z_TS', 'delta_x', 'delta_y', 'delta_z', 'Norm'])\n",
    "ts_untransformed = raw_data[raw_data.index.str.startswith('01_02')].values\n",
    "ts_points_transformed = ((R_opt @ ts_untransformed.T).T + t_opt)*1000\n",
    "ts_points_transformed = pd.DataFrame(ts_points_transformed, columns=['x_TS', 'y_TS', 'z_TS'], index=raw_data[raw_data.index.str.startswith('01_02')].index)\n",
    "data = data.join(ts_points_transformed) \n",
    "\n",
    "# Berechnung der Abweichungen zwischen Totalstation und Modellkoordinaten\n",
    "data['delta_x'] = data['x_TS'] - data['x_Encoder']\n",
    "data['delta_y'] = data['y_TS'] - data['y_Encoder']\n",
    "data['delta_z'] = data['z_TS'] - data['z_Encoder']\n",
    "data['Norm'] = np.linalg.norm(data[['delta_x', 'delta_y', 'delta_z']].values, axis=1)\n",
    "data.to_csv('../results/Tabellen/Modellentwicklung_Messwerte_transformiert.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot zur Darstellung des Einflusses der einzelnen Gelenke auf die Abweichungen $\\Delta{x}$, $\\Delta{y}$ und $\\Delta{z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot zur Gegenüberstellung der Encoderwerte und der Abweichungen\n",
    "sns.pairplot(data,    \n",
    "    x_vars=[\"encoder_0\", \"encoder_1\", \"encoder_2\", \"encoder_3\", \"encoder_4\", \"encoder_5\"],\n",
    "    y_vars=[\"delta_x\", \"delta_y\", \"delta_z\"],\n",
    "        )\n",
    "plt.savefig('../results/Diagramme/Modellentwicklung/Modellentwicklung_Pairplot_ohne_Korrektur.pdf')  # <<< PDF speichern\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korrelationsmatrix (Heat Map) für die Encoderwerte und die Abweichungen $\\Delta{x}$, $\\Delta{y}$ und $\\Delta{z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projektion des Punktvektors auf die XY-Ebene\n",
    "data['xy_projected'] = np.sqrt(data['x_Encoder']**2 + data['y_Encoder']**2)\n",
    "\n",
    "# Definieren der Encoder und der Abweichungen\n",
    "encoders = [f'encoder_{i}' for i in range(6)]\n",
    "encoder_correlation = ['xy_projected'] + [f'encoder_{i}' for i in range(6)]\n",
    "deltas = ['delta_x', 'delta_y', 'delta_z']\n",
    "encoder_labels = [r'\\textit{xy-projiziert}'] + [r'\\textit{Gelenk 0}', r'\\textit{Gelenk 1}', r'\\textit{Gelenk 2}', r'\\textit{Gelenk 3}', r'\\textit{Gelenk 4}', r'\\textit{Gelenk 5}']\n",
    "\n",
    "# Korrelation berechnen\n",
    "correlation = data[encoder_correlation + deltas].corr()\n",
    "\n",
    "# Korrelationen zwischen Encodern und projizierten Vektor und Deltas extrahieren\n",
    "correlation_subset = correlation.loc[deltas, encoder_correlation]\n",
    "\n",
    "# Heatmap plotten\n",
    "plt.figure(figsize=(9,4))\n",
    "ax = sns.heatmap(\n",
    "    correlation_subset,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    xticklabels=encoder_labels, \n",
    "    yticklabels=[r'\\textit{$\\Delta$x}', r'\\textit{$\\Delta$y}', r'\\textit{$\\Delta$z}']\n",
    ")\n",
    "\n",
    "# Kursive Achsenlabels setzen\n",
    "for label in ax.get_yticklabels():\n",
    "    label.set_fontstyle('italic')\n",
    "\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_fontstyle('italic')\n",
    "\n",
    "# Achsen beschriften\n",
    "plt.xlabel(\"Encoderstellung\")\n",
    "plt.ylabel(\"Abweichungen\")\n",
    "plt.savefig('../results/Diagramme/Modellentwicklung/Modellentwicklung_Korrelationsmatrix.pdf') \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnung eines linearen Modells zur Vorkorrektur der Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error linear Model (RMSE): 14.43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../results/Modelle/Lineare_Regression_Modell.pkl']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definieren der Prädiktoren und Zielvariablen\n",
    "predictors_linear = ['xy_projected']\n",
    "predicted_variables_linear = ['delta_z']\n",
    "\n",
    "# Extrahieren der Prädiktoren und Zielvariablen Messwerte\n",
    "X_linear = data[predictors_linear]  # Prädiktoren\n",
    "y_linear = data[predicted_variables_linear]  # Zielvariablen\n",
    "\n",
    "# Transformieren der Prädiktoren für polynomische Eigenschaften\n",
    "poly = PolynomialFeatures(degree=2)  # Polynomgrad 2\n",
    "X_linear = poly.fit_transform(X_linear) \n",
    "\n",
    "# Trennen der Daten in Trainings- und Testdaten (80% Training, 20% Test)\n",
    "X_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(X_linear, y_linear, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training des Modells\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(X_train_linear, y_train_linear)\n",
    "\n",
    "# Vorhersagen des Modells bei den Testdaten\n",
    "y_pred_linear = model_linear.predict(X_test_linear)\n",
    "\n",
    "# Modellbewertung\n",
    "norm_mean_linear = np.linalg.norm(y_test_linear - y_pred_linear).mean()\n",
    "print(f\"Root Mean Squared Error linear Model (RMSE): {np.sqrt(norm_mean_linear):.2f}\")\n",
    "\n",
    "# Ermittlung der Korrekturwerte aller Messwerte mit dem linearen Modell und Berechnung der neuen korrigierten Koordinaten\n",
    "y_pred_total_linear = model_linear.predict(poly.transform(data[predictors_linear]))\n",
    "variables_new = [var + \"_linear\" for var in predicted_variables_linear]\n",
    "data[variables_new] = data[predicted_variables_linear] - y_pred_total_linear\n",
    "\n",
    "# Speichern des Modells als Pkl-Datei\n",
    "joblib.dump(model_linear, '../results/Modelle/Lineare_Regression_Modell.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotten des polynomischen Modells zur Überprüfung der Vorhersagestetigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\si159\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Erstellen einer Reihe von Werten zwischen 0 und 12000\n",
    "x_range = np.linspace(0, 12000, 500).reshape(-1, 1)  # 500 Punkte für eine glatte Kurve\n",
    "\n",
    "# Transformieren der 500 Werte, um die polynomiale Regression zu berücksichtigen\n",
    "x_range_poly = poly.transform(x_range)\n",
    "\n",
    "# Vorhersage der Korrektur mit dem trainierten linearem Regressions-Modell\n",
    "y_pred_range = model_linear.predict(x_range_poly)\n",
    "\n",
    "# Plotten der Vorhersagekurve\n",
    "plt.scatter(data['xy_projected'], data['delta_z'], color='blue', alpha=0.3, label=\"Echte Werte\")  # Originaldaten\n",
    "plt.plot(x_range, y_pred_range, color='red', label=\"Polynomiale Vorhersage\")  # Modellvorhersage\n",
    "plt.xlabel(r'\\textit{xy-projiziert} in mm')\n",
    "plt.ylabel(r'\\textit{$\\Delta$z} in mm')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('../results/Diagramme/Modellentwicklung/Modellentwicklung_lineare_Regression.pdf') \n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorbereitung für die Berechnung des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auswahl der Trainingskriterien und der Zielvariablen\n",
    "columns = ['delta_x', 'delta_y', 'delta_z']\n",
    "y_labels = [col + '_linear' if col + '_linear' in data.columns else col for col in columns]\n",
    "X = data[encoders]  \n",
    "y = data[y_labels]\n",
    "\n",
    "# Aufteilen des Datensatzes in Test und Trainigsdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search zur Hyperparameteroptimierung des Random Forest Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Definieren der Werte für die Hyperparameter\\nparam_grid = {\\n    \\'n_estimators\\': [50, 100, 200],\\n    \\'max_depth\\': [10, 20, 1],\\n    \\'min_samples_split\\': [2, 5, 10],\\n    \\'min_samples_leaf\\': [1, 2, 4],\\n    \\'max_features\\': [None, \\'sqrt\\', \\'log2\\']\\n}\\n\\n# Durchführen des GridSearchs zur Hyperparameteroptimierung\\ngrid_search = GridSearchCV(\\n    estimator=RandomForestRegressor(random_state=42),\\n    param_grid=param_grid,\\n    scoring=\\'neg_mean_squared_error\\',\\n    cv=5,\\n    n_jobs=-1,\\n    verbose=2\\n)\\n\\n# Training des Modells mit den aktuellen Hyperparametern\\ngrid_search.fit(X_train, y_train)\\n\\nprint(\"Best Parameters:\", grid_search.best_params_)'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Definieren der Werte für die Hyperparameter\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 1],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Durchführen des GridSearchs zur Hyperparameteroptimierung\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Training des Modells mit den aktuellen Hyperparametern\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainieren des Random Forest Modells auf die vorkorrigierten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/Modelle/Random_Forest_Modell.pkl']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainieren des Modells\n",
    "model = RandomForestRegressor(n_estimators=200, random_state=42, min_samples_leaf=1, min_samples_split=2, max_features=None, max_depth=20)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Speichern des Modells als Pkl-Datei\n",
    "joblib.dump(model, '../results/Modelle/Random_Forest_Modell.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluierung der Vorhersagegenauigkeit des Modells durch die Anwendung des Modells auf den Test Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prediction:  0.029831409454345703\n",
      "Mittlere Abweichung der Norm : 6.48\n"
     ]
    }
   ],
   "source": [
    "# Laufzeitmessung für die Vorhersage\n",
    "start_time = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for prediction: \", end_time - start_time)\n",
    "\n",
    "# Vorhersage der Korrekturwerte für alle Messwerte\n",
    "test_data = pd.concat([X_test, y_test, data[['x_Encoder', 'y_Encoder', 'z_Encoder']]], axis=1, join='inner')\n",
    "coordinates_test_new = test_data[['x_Encoder', 'y_Encoder', 'z_Encoder']] + y_pred\n",
    "deltas_test_new = y_test - y_pred\n",
    "\n",
    "# Berechnung der Norm der Abweichungen und des entsprechenden Mittelwertes\n",
    "norm_mean_RF = np.sqrt(deltas_test_new['delta_x']**2 + deltas_test_new['delta_y']**2 + deltas_test_new['delta_z_linear']**2).mean()\n",
    "print(f\"Mittlere Abweichung der Norm : {norm_mean_RF:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellung eines Vektorplots zum Vergleich der Abweichungen der Testdaten vor und nach der Modellanwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren der Abweichungen\n",
    "scaled_errors_before_test = y_test\n",
    "scaled_errors_after_test = deltas_test_new\n",
    "\n",
    "sizeref_factor = 10  # Skalierung der Abweichungen für die Visualisierung festlegen\n",
    "\n",
    "# Messpunkte und Abweichungen vor der Korrektur (Testdaten)\n",
    "quivers_test_before = go.Cone(\n",
    "    x=test_data[\"x_Encoder\"],\n",
    "    y=test_data[\"y_Encoder\"],\n",
    "    z=test_data[\"z_Encoder\"],\n",
    "    u=scaled_errors_before_test.iloc[:, 0],\n",
    "    v=scaled_errors_before_test.iloc[:, 1],\n",
    "    w=scaled_errors_before_test.iloc[:, 2],\n",
    "    opacity= 0.3,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor,  \n",
    "    colorscale='Reds',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br>vor Optimierung in mm\",\n",
    "        len=0.3,  # Länge der Farbskala\n",
    "        x=0.85,   # x-Position\n",
    "        y=0.9    # y-Position\n",
    "    ),\n",
    "    name='Vorher'\n",
    ")\n",
    "\n",
    "# Messpunkte und Abweichungen nach der Korrektur (Testdaten)\n",
    "quivers_test_after = go.Cone(\n",
    "    x=coordinates_test_new[\"x_Encoder\"],\n",
    "    y=coordinates_test_new[\"y_Encoder\"],\n",
    "    z=coordinates_test_new[\"z_Encoder\"],\n",
    "    u=scaled_errors_after_test.iloc[:, 0],\n",
    "    v=scaled_errors_after_test.iloc[:, 1],\n",
    "    w=scaled_errors_after_test.iloc[:, 2],\n",
    "    opacity= 0.6,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor,\n",
    "    colorscale='Greens',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br>nach Optimierung in mm\",\n",
    "        len=0.3,  # Länge der Farbskala\n",
    "        x=0.85,   # x-Position\n",
    "        y=0.6    # y-Position\n",
    "    ),\n",
    "    name='Nachher'\n",
    ")\n",
    "\n",
    "\n",
    "# Erstellen der 3D-Visualisierung\n",
    "fig = go.Figure(data=[quivers_test_before, quivers_test_after])\n",
    "\n",
    "# Anpassung des Layouts\n",
    "fig.update_layout(\n",
    "    title='3D Vektorplot Testdaten vor und nach Korrektur',\n",
    "    scene=dict(\n",
    "        xaxis_title=\"<i>x</i> in mm\",\n",
    "        yaxis_title='<i>y</i> in mm',\n",
    "        zaxis_title='<i>z</i> in mm'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Speichern der Visualisierung als HTML-Datei\n",
    "fig.write_html(\"../results/Diagramme/Modellentwicklung/3D_Vektorplot_Testdaten_Korrektur.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellung einer Lernkurve anhand der Verwendung unterschiedlicher Anzahl an Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Lernkurve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y, cv=5, scoring='neg_mean_squared_error', train_sizes=np.linspace(0.1, 1.0, 10), random_state=42\n",
    ")\n",
    "\n",
    "# Berechnung der Mittelwerte und Standardabweichungen\n",
    "train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plotten der Lernkurve\n",
    "plt.figure()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Trainigsdaten\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Validierungsdaten\")\n",
    "plt.xlabel(\"Anzahl an Trainingsdaten\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig('../results/Diagramme/Modellentwicklung/Modellentwicklung_Lernkurve.pdf') \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ermittlung der größten Einflussfaktoren für das Modell und Darstellung anhand eines Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Importances berechnen\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "x_Labels = [r'\\textit{Encoder 0}', r'\\textit{Encoder 1}', r'\\textit{Encoder 2}', r'\\textit{Encoder 3}', r'\\textit{Encoder 4}', r'\\textit{Encoder 5}']\n",
    "\n",
    "# Plotten der Feature-Importances\n",
    "plt.figure()\n",
    "plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), [x_Labels[i] for i in indices])\n",
    "plt.xlabel(\"Einflussfaktoren\")\n",
    "plt.ylabel(\"Gewichtung\")\n",
    "plt.savefig('../results/Diagramme/Modellentwicklung/Modellentwicklung_Input_Parameter_Einfluss.pdf') \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vektorplot zur Überprüfung der Vorhersagestetigkeit (Vorhersagewerte anhand eines Vektorplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Messwerte mit dem Random Forest Modell vorhersagen\n",
    "X_total = data[encoders] \n",
    "y_pred_total = pd.DataFrame(model.predict(X_total))\n",
    "\n",
    "# Vorhersage als Vektorlänge definieren\n",
    "scaled_errors_before = y_pred_total\n",
    "sizeref_factor = 10  # Skalierfaktor für die Vektorlängen\n",
    "\n",
    "# Vektoren der Vorhersage an jedem Punkt\n",
    "quivers_total = go.Cone(\n",
    "    x=data[\"x_Encoder\"],\n",
    "    y=data[\"y_Encoder\"],\n",
    "    z=data[\"z_Encoder\"],\n",
    "    u=scaled_errors_before.iloc[:, 0],\n",
    "    v=scaled_errors_before.iloc[:, 1],\n",
    "    w=scaled_errors_before.iloc[:, 2],\n",
    "    opacity= 1,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor, \n",
    "    colorscale='Reds',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br> Vorhersage in mm\",\n",
    "        len=0.5, \n",
    "        x=0.85,   \n",
    "        y=0.6    \n",
    "    )\n",
    ")\n",
    "\n",
    "# Erstellen des Vektorplots\n",
    "fig = go.Figure(data=[quivers_total])\n",
    "\n",
    "# Layout anpassen\n",
    "fig.update_layout(\n",
    "    title='3D Vektorplot Prüfung der Vorhersagestetigkeit',\n",
    " scene=dict(\n",
    "        xaxis_title=\"<i>x</i> in mm\",\n",
    "        yaxis_title='<i>y</i> in mm',\n",
    "        zaxis_title='<i>z</i> in mm'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Speichern des Plots als HTML-Datei\n",
    "fig.write_html(\"../results/Diagramme/Modellentwicklung/3D_Vektorplot_Vorhersage_Stetigkeit_RF.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnung der mittleren Fehler der Norm nach jeder Korrekturstufe (alle Messwerte, Training + Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mittelwert der Fehlernorm vor Modellanwendung auf alle Messwerte (Training + Testdaten): 32.016458175734435\n",
      "Mittelwert der Fehlernorm nach linearer Regression aller Messwerte (Training + Testdaten): 21.079585264280436\n",
      "Mittelwert der Fehlernorm nach Random Forest aller Messwerte (Training + Testdaten): 3.145769671171821\n"
     ]
    }
   ],
   "source": [
    "# Berechnung der Fehlernorm vor der Modellanwendung\n",
    "norm_mean_before_model = np.linalg.norm(data[['delta_x', 'delta_y', 'delta_z']].values, axis=1).mean()\n",
    "print(f\"Mittelwert der Fehlernorm vor Modellanwendung auf alle Messwerte (Training + Testdaten): {norm_mean_before_model}\")\n",
    "\n",
    "# Berechnung der Fehlernorm nach der Modellanwendung des linearen Modells\n",
    "columns = ['x_Encoder', 'y_Encoder', 'z_Encoder']\n",
    "variables_new_Encoder_linear = [col + '_linear' for col in columns if 'delta_' + col[0] in predicted_variables_linear]\n",
    "data[variables_new_Encoder_linear] = data[[col for col in columns if 'delta_' + col[0] in predicted_variables_linear]] + y_pred_total_linear\n",
    "labels_linear_plot = [col + '_linear' if 'delta_' + col[0] in predicted_variables_linear else col for col in columns] + y_labels\n",
    "data_linear_plot = data[labels_linear_plot]\n",
    "norm_mean_after_linear = np.linalg.norm(data[y_labels].values, axis=1).mean()\n",
    "print(f\"Mittelwert der Fehlernorm nach linearer Regression aller Messwerte (Training + Testdaten): {norm_mean_after_linear}\")\n",
    "\n",
    "# Berechnung der Fehlernorm nach der Modellanwendung des linearen Modells und des Random Forest Modells\n",
    "y_RF_data = pd.DataFrame(model.predict(data[encoders]),columns= y_labels)\n",
    "y_RF_data[variables_new] = y_RF_data[variables_new] + y_pred_total_linear\n",
    "coordinates_new_RF = data[['x_Encoder','y_Encoder','z_Encoder']] + y_RF_data.values\n",
    "deltas_RF_final = data[['delta_x', 'delta_y', 'delta_z']] - y_RF_data.values\n",
    "norm_mean_after_RF = np.linalg.norm(deltas_RF_final.values, axis=1).mean()\n",
    "print(f\"Mittelwert der Fehlernorm nach Random Forest aller Messwerte (Training + Testdaten): {norm_mean_after_RF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleich der unkorrigierten, linear korrigierten und vollständig korrigierten Werte aller Messwerte (Training + Testdaten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehlervektoren jeder Datenreihe definieren\n",
    "scaled_errors_data = data[['delta_x', 'delta_y', 'delta_z']]\n",
    "scaled_errors_linear = data[y_labels]\n",
    "scaled_errors_RF = deltas_RF_final\n",
    "\n",
    "sizeref_factor = 10  # Skalierungsfaktor für die Vektorlängen\n",
    "\n",
    "# Vektorplot vor der Korrektur\n",
    "quivers_all_before = go.Cone(\n",
    "    x=data[\"x_Encoder\"],\n",
    "    y=data[\"y_Encoder\"],\n",
    "    z=data[\"z_Encoder\"],\n",
    "    u=scaled_errors_data.iloc[:, 0],\n",
    "    v=scaled_errors_data.iloc[:, 1],\n",
    "    w=scaled_errors_data.iloc[:, 2],\n",
    "    opacity= 0.3,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor,  \n",
    "    colorscale='Reds',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br>unkorrigiert in mm\",\n",
    "        len=0.3,  \n",
    "        x=0.85,   \n",
    "        y=0.9    \n",
    "    ),\n",
    "    name='ohne Korrektur'\n",
    ")\n",
    "\n",
    "# Vektorplot nach der linearen Korrektur\n",
    "quivers_linear = go.Cone(\n",
    "    x=data_linear_plot.iloc[:,0],\n",
    "    y=data_linear_plot.iloc[:,1],\n",
    "    z=data_linear_plot.iloc[:,2],\n",
    "    u=data_linear_plot.iloc[:, 3],\n",
    "    v=data_linear_plot.iloc[:, 4],\n",
    "    w=data_linear_plot.iloc[:, 5],\n",
    "    opacity= 0.6,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor, \n",
    "    colorscale='Blues',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br>linear in mm\",\n",
    "        len=0.3,  \n",
    "        x=0.85,   \n",
    "        y=0.6   \n",
    "    ),\n",
    "    name='linear korrigiert'\n",
    ")\n",
    "   \n",
    "\n",
    "# Vektorplot nach linearer Korrektur und Random Forest Korrektur\n",
    "quivers_RF = go.Cone(\n",
    "    x=coordinates_new_RF[\"x_Encoder\"],\n",
    "    y=coordinates_new_RF[\"y_Encoder\"],\n",
    "    z=coordinates_new_RF[\"z_Encoder\"],\n",
    "    u=scaled_errors_RF.iloc[:, 0],\n",
    "    v=scaled_errors_RF.iloc[:, 1],\n",
    "    w=scaled_errors_RF.iloc[:, 2],\n",
    "    opacity= 0.8,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor,  \n",
    "    colorscale='Greens',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br>RF in mm\",\n",
    "        len=0.3, \n",
    "        x=0.85,   \n",
    "        y=0.3   \n",
    "    ),\n",
    "    name='linear + RF korrigiert'\n",
    ")\n",
    "\n",
    "\n",
    "# Erstellen der 3D-Visualisierung\n",
    "fig = go.Figure(data=[quivers_all_before, quivers_linear,quivers_RF])\n",
    "\n",
    "# Layout anpassen\n",
    "fig.update_layout(\n",
    "    title='3D Vektorplot Vergleich unkorrigiert, linear korrigiert und RF korrigiert',\n",
    "    scene=dict(\n",
    "        xaxis_title=\"<i>x</i> in mm\",\n",
    "        yaxis_title='<i>y</i> in mm',\n",
    "        zaxis_title='<i>z</i> in mm'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Speichern der Visualisierung als HTML-Datei\n",
    "fig.write_html(\"../results/Diagramme/Modellentwicklung/3D_Vektorplot_unkorrigiert_linear_RF.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validierung des Modells anhand von Messwerten eines anderen Versuchstages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Validierungsdaten für die Einmessung und die effektive Validierung\n",
    "data_TS_validation_positioning = pd.read_csv('../data/Modellentwicklung_Rohdaten_Totalstation_Validierung_Einmessung.csv', delimiter=';', index_col=0).sort_index()\n",
    "data_Encoder_validation_positioning = pd.read_csv('../data/Modellentwicklung_Encoderwerte_Validierung_Einmessung.csv', delimiter=';', index_col=0).sort_index()\n",
    "data_TS_validation = pd.read_csv('../data/Modellentwicklung_Rohdaten_Totalstation_Validierung.csv', delimiter=';', index_col=0).sort_index()\n",
    "data_Encoder_validation = pd.read_csv('../data/Modellentwicklung_Encoderwerte_Validierung.csv', delimiter=';', index_col=0,).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bestimmung der Transformation für die Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimale Rotation: [[-0.99942475  0.02326937  0.02467202]\n",
      " [-0.02288121 -0.99961174  0.01590018]\n",
      " [ 0.02503243  0.01532651  0.99956915]]\n",
      "Optimale Translation: [ 954.20442903 2009.92486642 -155.43326459]\n",
      "Eulerwinkel in Grad: [   1.31152232 -178.56559777  180.87845408]\n",
      "Mittlerer Fehler der Norm, nach Transformation: 5.082632831283495\n"
     ]
    }
   ],
   "source": [
    "P = data_TS_validation_positioning[['x_TS','y_TS','z_TS']].values\n",
    "Q = data_Encoder_validation_positioning[['x_Encoder','y_Encoder','z_Encoder']].values/1000\n",
    "\n",
    "# Erster Schritt: Kabsch-Algorithmus zur Initialschätzung\n",
    "R_est, t_est = kabsch_algorithm(P, Q)\n",
    "\n",
    "# Initialschätzung für die Optimierung\n",
    "x0 = np.array([0.0, 0.0, 0.0, t_est[0], t_est[1], t_est[2]])\n",
    "\n",
    "# Robuste Optimierung mit Huber-Loss\n",
    "result = least_squares(residual, x0, args=(P, Q), loss='cauchy', f_scale=1.0)\n",
    "alpha_opt, beta_opt, gamma_opt, tx_opt, ty_opt, tz_opt = result.x\n",
    "residuals_after_kabsch_local = residual(result.x, P, Q)\n",
    "R_opt = rotation_matrix_from_euler(alpha_opt, beta_opt, gamma_opt)\n",
    "t_opt = np.array([tx_opt, ty_opt, tz_opt])\n",
    "print(\"Optimale Rotation:\", R_opt)\n",
    "print(\"Optimale Translation:\", t_opt)\n",
    "print(\"Eulerwinkel in Grad:\", np.degrees(result.x[0:3]))\n",
    "\n",
    "# Mittlerer Fehler der Norm nach der Optimierung\n",
    "err_final = np.linalg.norm((R_opt @ P.T).T + t_opt - Q, axis=1)\n",
    "err_final_mean = np.mean(err_final)\n",
    "err_final_mean_squared = np.sqrt(np.mean((((R_opt @ P.T).T + t_opt) - Q)**2))\n",
    "print(\"Mittlerer Fehler der Norm, nach Transformation:\", err_final_mean*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation des Validierungsdatensatzes und Umrechnung der Encoderrohdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation der Totalstation-Messwerte\n",
    "ts_validation_untransformed = data_TS_validation.values\n",
    "ts_points_transformed = ((R_opt @ ts_validation_untransformed.T).T + t_opt)*1000\n",
    "df_validation = pd.DataFrame(ts_points_transformed, columns=['x_TS', 'y_TS', 'z_TS'], index=data_TS_validation.index)\n",
    "df_validation = df_validation.join(data_Encoder_validation) \n",
    "\n",
    "# Abweichungen und andere Parameter berechnen\n",
    "df_validation['delta_x'] = df_validation['x_TS'] - df_validation['x_Encoder']\n",
    "df_validation['delta_y'] = df_validation['y_TS'] - df_validation['y_Encoder']\n",
    "df_validation['delta_z'] = df_validation['z_TS'] - df_validation['z_Encoder']\n",
    "df_validation['Norm'] = np.linalg.norm(df_validation[['delta_x', 'delta_y', 'delta_z']].values, axis=1)\n",
    "df_validation['xy_projected'] = np.sqrt(df_validation['x_Encoder']**2 + df_validation['y_Encoder']**2)\n",
    "df_validation['vector_norm'] = np.sqrt(df_validation['x_Encoder']**2 + df_validation['y_Encoder']**2 + df_validation['z_Encoder']**2)\n",
    "\n",
    "# Umrechnung der Encoderwerte von Rohdaten in Winkelwerte\n",
    "df_validation['encoder_0'] = gelenk0_TF(df_validation['encoder_0'])\n",
    "df_validation['encoder_1'] = gelenk1_TF(df_validation['encoder_1'])\n",
    "df_validation['encoder_2'] = gelenk2_TF(df_validation['encoder_2'])\n",
    "df_validation['encoder_3'] = gelenk3_TF(df_validation['encoder_3'])\n",
    "df_validation['encoder_4'] = gelenk4_TF(df_validation['encoder_4'])\n",
    "df_validation['encoder_5'] = gelenk5_TF(df_validation['encoder_5'])\n",
    "df_validation['encoder_6'] = gelenk6_TF(df_validation['encoder_6'])\n",
    "df_validation['encoder_7'] = gelenk7_TF(df_validation['encoder_7'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnung der mittleren Fehler der Norm der Validierungsdaten (mit gefüllten Betonschläuchen an einem anderen Tag aufgenommen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mittelwert der Fehlernorm vor Modellanwendung auf alle Messwerte (Validierung): 27.59791996897788\n",
      "Mittelwert der Fehlernorm nach linearer Korrektur aller Messwerte (Validierung): 25.345423448583666\n",
      "Mittelwert der Fehlernorm nach linearer und Random Forest Korrektur aller Messwerte (Validierung): 22.008820712017865\n"
     ]
    }
   ],
   "source": [
    "# Berechnung der mittleren Fehlernorm vor der Modellanwendung\n",
    "norm_mean_before_model_validation = np.linalg.norm(df_validation[['delta_x', 'delta_y', 'delta_z']].values, axis=1).mean()\n",
    "print(f\"Mittelwert der Fehlernorm vor Modellanwendung auf alle Messwerte (Validierung): {norm_mean_before_model_validation}\")\n",
    "\n",
    "# Berechnung der Fehlernorm nach der Modellanwendung des linearen Modells\n",
    "y_pred_validation_linear = model_linear.predict(poly.transform(df_validation[predictors_linear]))\n",
    "df_validation[variables_new] = df_validation[predicted_variables_linear] - y_pred_validation_linear\n",
    "columns = ['x_Encoder', 'y_Encoder', 'z_Encoder']\n",
    "variables_new_Encoder_linear_validation = [col + '_linear' for col in columns if 'delta_' + col[0] in predicted_variables_linear]\n",
    "df_validation[variables_new_Encoder_linear_validation] = df_validation[[col for col in columns if 'delta_' + col[0] in predicted_variables_linear]] + y_pred_validation_linear\n",
    "data_linear_plot_validation = df_validation[labels_linear_plot]\n",
    "norm_mean_after_linear_validation = np.linalg.norm(df_validation[y_labels].values, axis=1).mean()\n",
    "print(f\"Mittelwert der Fehlernorm nach linearer Korrektur aller Messwerte (Validierung): {norm_mean_after_linear_validation}\")\n",
    "\n",
    "# Berechnung der mittleren Fehlernorm nach der Modellanwendung des linearen Modells und des Random Forest Modells\n",
    "y_RF_data_validation = pd.DataFrame(model.predict(df_validation[encoders]),columns= y_labels)\n",
    "y_RF_data_validation[variables_new] = y_RF_data_validation[variables_new] + y_pred_validation_linear\n",
    "coordinates_new_RF_validation = df_validation[['x_Encoder','y_Encoder','z_Encoder']] + y_RF_data_validation.values\n",
    "deltas_RF_final_validation = df_validation[['delta_x', 'delta_y', 'delta_z']] - y_RF_data_validation.values\n",
    "norm_mean_after_RF_validation = np.linalg.norm(deltas_RF_final_validation.values, axis=1).mean()\n",
    "print(f\"Mittelwert der Fehlernorm nach linearer und Random Forest Korrektur aller Messwerte (Validierung): {norm_mean_after_RF_validation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darstellung des Vektorplots der unkorrigierten und korrigierten Werte des Validierungsdatensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehlervektoren jeder Datenreihe definieren\n",
    "scaled_errors_data = df_validation[['delta_x', 'delta_y', 'delta_z']]\n",
    "scaled_errors_linear = df_validation[y_labels]\n",
    "scaled_errors_RF = deltas_RF_final_validation\n",
    "\n",
    "sizeref_factor = 10  # Skalierungsfaktor für die Vektorlängen\n",
    "\n",
    "# Vektorplot vor der Korrektur (Validierung)\n",
    "quivers_all_before = go.Cone(\n",
    "    x=df_validation[\"x_Encoder\"],\n",
    "    y=df_validation[\"y_Encoder\"],\n",
    "    z=df_validation[\"z_Encoder\"],\n",
    "    u=scaled_errors_data.iloc[:, 0],\n",
    "    v=scaled_errors_data.iloc[:, 1],\n",
    "    w=scaled_errors_data.iloc[:, 2],\n",
    "    opacity= 0.3,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor,  \n",
    "    colorscale='Reds',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br>unkorrigiert in mm\",\n",
    "        len=0.3,  \n",
    "        x=0.85,  \n",
    "        y=0.9  \n",
    "    ),\n",
    "    name='ohne Korrektur'\n",
    ")\n",
    "\n",
    "# Create vectors (quivers) for errors before transformation\n",
    "quivers_linear = go.Cone(\n",
    "    x=data_linear_plot_validation.iloc[:,0],\n",
    "    y=data_linear_plot_validation.iloc[:,1],\n",
    "    z=data_linear_plot_validation.iloc[:,2],\n",
    "    u=data_linear_plot_validation.iloc[:, 3],\n",
    "    v=data_linear_plot_validation.iloc[:, 4],\n",
    "    w=data_linear_plot_validation.iloc[:, 5],\n",
    "    opacity= 0.6,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor,  # Use calculated sizeref for errors before\n",
    "    colorscale='Blues',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br>poly in mm\",\n",
    "        len=0.3,  # Length of the colorbar\n",
    "        x=0.85,   # Position on the x-axis\n",
    "        y=0.6    # Position on the y-axis\n",
    "    ),\n",
    "    name='linear korrigiert'\n",
    ")\n",
    "   \n",
    "\n",
    "# Create vectors (quivers) for errors before transformation\n",
    "quivers_RF = go.Cone(\n",
    "    x=coordinates_new_RF_validation[\"x_Encoder\"],\n",
    "    y=coordinates_new_RF_validation[\"y_Encoder\"],\n",
    "    z=coordinates_new_RF_validation[\"z_Encoder\"],\n",
    "    u=scaled_errors_RF.iloc[:, 0],\n",
    "    v=scaled_errors_RF.iloc[:, 1],\n",
    "    w=scaled_errors_RF.iloc[:, 2],\n",
    "    opacity= 0.8,\n",
    "    sizemode=\"raw\",\n",
    "    sizeref=sizeref_factor,  # Use calculated sizeref for errors before\n",
    "    colorscale='Greens',\n",
    "    colorbar=dict(\n",
    "        title=\"Abweichung<br>RF in mm\",\n",
    "        len=0.3,  # Length of the colorbar\n",
    "        x=0.85,   # Position on the x-axis\n",
    "        y=0.3    # Position on the y-axis\n",
    "    ),\n",
    "    name='linear + RF korrigiert'\n",
    ")\n",
    "\n",
    "\n",
    "# Create the figure and add traces\n",
    "fig = go.Figure(data=[quivers_all_before, quivers_linear,quivers_RF])\n",
    "\n",
    "# Set plot layout\n",
    "fig.update_layout(\n",
    "    title='3D Vektorplot Vergleich Validierungsdaten unkorrigiert, linear korrigiert und RF korrigiert',\n",
    "    scene=dict(\n",
    "        xaxis_title=\"<i>x</i> in mm\",\n",
    "        yaxis_title='<i>y</i> in mm',\n",
    "        zaxis_title='<i>z</i> in mm'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the plot to an HTML file\n",
    "fig.write_html(\"../results/Diagramme/Modellentwicklung/3D_Vektorplot_unkorrigiert_linear_RF_Validierung.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
