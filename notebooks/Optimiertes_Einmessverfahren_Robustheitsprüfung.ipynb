{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1059b835",
   "metadata": {},
   "source": [
    "Packages importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eccdeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "from scipy.optimize import least_squares\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib import rcParams\n",
    "import locale\n",
    "\n",
    "# === 1. Globale Einstellungen für ALLE Plots ===\n",
    "rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Latin Modern Sans\"],\n",
    "\n",
    "    # Lade lmodern und sansmath, aktiviere serifenlose Mathematik\n",
    "    \"text.latex.preamble\": r\"\"\"\n",
    "        \\usepackage{lmodern}\n",
    "        \\usepackage{sansmath}\n",
    "        \\sansmath\n",
    "        \\renewcommand{\\familydefault}{\\sfdefault}\n",
    "        \\usepackage{icomma}\n",
    "    \"\"\",\n",
    "\n",
    "     # Größen etc. wie gehabt\n",
    "    \"font.size\": 10,              # Standard-Schriftgröße für Paper\n",
    "    \"axes.labelsize\": 10,          # Achsentitelgröße\n",
    "    \"axes.titlesize\": 12,          # Plottitelgröße\n",
    "    \"legend.fontsize\": 9,          # Legendentext\n",
    "    \"xtick.labelsize\": 9,          # Tick-Label-Größe X\n",
    "    \"ytick.labelsize\": 9,          # Tick-Label-Größe Y\n",
    "\n",
    "    # Plotgröße und Auflösung\n",
    "    \"figure.figsize\": (6, 4),      # etwa 15x10 cm für LaTeX-Spaltenbreite\n",
    "    \"figure.dpi\": 300,\n",
    "\n",
    "    # Ränder\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \n",
    "    # Linien und Gitter\n",
    "    \"axes.grid\": True,             # Gitter standardmäßig an\n",
    "    \"grid.color\": \"gray\",\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \n",
    "    # Achsenlinien\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \n",
    "    # Ticks\n",
    "    \"xtick.direction\": \"in\",       # Ticks nach innen\n",
    "    \"ytick.direction\": \"in\",\n",
    "    \"xtick.major.size\": 5,         # Ticklänge\n",
    "    \"ytick.major.size\": 5,\n",
    "    \"xtick.minor.size\": 2.5,       # Kleine Ticks\n",
    "    \"ytick.minor.size\": 2.5,\n",
    "})\n",
    "\n",
    "# 1) Deutsche Locale setzen (ggf. liste mit locale -a prüfen)\n",
    "locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8')\n",
    "\n",
    "# 2) Matplotlib sagen, die Locale zu benutzen\n",
    "rcParams['axes.formatter.use_locale'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996d784",
   "metadata": {},
   "source": [
    "Funktionen importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b90398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch_algorithm(P, Q):\n",
    "    p_centroid = np.mean(P, axis=0)\n",
    "    q_centroid = np.mean(Q, axis=0)\n",
    "    P_centered = P - p_centroid # Werte werden ins Zentrum des Koordinatensystems verschoben\n",
    "    Q_centered = Q - q_centroid # Werte werden ins Zentrum des Koordinatensystems verschoben\n",
    "    H = P_centered.T @ Q_centered # Korrelationsmatrix\n",
    "    U, S, Vt = svd(H)\n",
    "    V = Vt.T\n",
    "    R = V @ U.T\n",
    "    if np.linalg.det(R) < 0:\n",
    "        V[:,-1] *= -1\n",
    "        R = V @ U.T\n",
    "    t = q_centroid - R @ p_centroid\n",
    "    return R, t\n",
    "\n",
    "def rotation_matrix_from_euler(alpha, beta, gamma):\n",
    "    # Rotationsmatrizen für Eulerwinkel (Z-Y-X)\n",
    "    Rz = np.array([[np.cos(alpha), -np.sin(alpha), 0],\n",
    "                   [np.sin(alpha),  np.cos(alpha), 0],\n",
    "                   [0,              0,             1]])\n",
    "    Ry = np.array([[ np.cos(beta), 0, np.sin(beta)],\n",
    "                   [0,             1, 0],\n",
    "                   [-np.sin(beta), 0, np.cos(beta)]])\n",
    "    Rx = np.array([[1,             0,              0],\n",
    "                   [0, np.cos(gamma), -np.sin(gamma)],\n",
    "                   [0, np.sin(gamma),  np.cos(gamma)]])\n",
    "    return Rz @ Ry @ Rx\n",
    "\n",
    "def residual(params, P, Q):\n",
    "    alpha, beta, gamma, tx, ty, tz = params\n",
    "    R_opt = rotation_matrix_from_euler(alpha, beta, gamma)\n",
    "    t_opt = np.array([tx, ty, tz])\n",
    "    P_transformed = (R_opt @ P.T).T + t_opt\n",
    "    diff = P_transformed - Q\n",
    "    return diff.flatten()\n",
    "\n",
    "# Introduce noise into 10 random data points\n",
    "def add_noise_to_random_points(data, noise_level=0.01, num_points=10):\n",
    "    indices = np.random.choice(data.shape[0], num_points, replace=False)\n",
    "    noise = np.random.normal(0, noise_level, (num_points, data.shape[1]))\n",
    "    data_noisy = data.copy()\n",
    "    data_noisy[indices] += noise\n",
    "    return data_noisy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529d50f",
   "metadata": {},
   "source": [
    "Messdaten aller Messtage 21.01., 01.02., 10.02., 03.03. (Position 1) und 03.03. (Position 2) importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09e00b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten vom 21.01.2025 importieren und strukturieren\n",
    "data_ts_21_01 = pd.read_csv('../data/Rohdaten_TS_21_01_25.csv', delimiter=';', index_col=0).sort_index()\n",
    "data_encoder_21_01 = pd.read_csv('../data/Encoderwerte_21_01_25.csv', delimiter=';', index_col=0).sort_index()\n",
    "validation_data_TS_21_01 = pd.read_csv('../data/Rohdaten_TS_21_01_V_25.csv', delimiter=';', index_col=0).sort_index()\n",
    "validation_data_encoder_21_01 = pd.read_csv('../data/Encoderwerte_21_01_V_25.csv', delimiter=';', index_col=0).sort_index()\n",
    "\n",
    "encoder_points_21_01 = (data_encoder_21_01[['x_Encoder', 'y_Encoder', 'z_Encoder']]/1000)\n",
    "ts_points_21_01 = data_ts_21_01[['x_TS', 'y_TS', 'z_TS']]\n",
    "\n",
    "encoder_points_validation_21_01 = validation_data_encoder_21_01[['x_Encoder', 'y_Encoder', 'z_Encoder']].values/1000\n",
    "ts_points_validation_21_01 = validation_data_TS_21_01[['x_TS', 'y_TS', 'z_TS']].values \n",
    "\n",
    "# Daten vom 01.02.2025 importieren und strukturieren\n",
    "data = pd.read_csv('../data/Modellentwicklung_Encoderwerte.csv', sep=';', index_col=0)\n",
    "raw_data = pd.read_csv('../data/Modellentwicklung_Rohdaten_Totalstation.csv', delimiter=';', index_col=0)\n",
    "\n",
    "raw_data.columns = ['x_TS', 'y_TS', 'z_TS']\n",
    "data_positioning = data[\n",
    "    (data['Norm'] < 20) & \n",
    "    (data['x_Encoder'] < 2000) & \n",
    "    (data['x_Encoder'] > -1200) & \n",
    "    (data['y_Encoder'] < 8000) & \n",
    "    (data['z_Encoder'] > -1000) & \n",
    "    (data['z_Encoder'] < 2200)\n",
    "]\n",
    "data_positioning.index = data_positioning.index\n",
    "raw_data_positioning = raw_data[raw_data.index.isin(data_positioning.index)]\n",
    "ts_points_01_02 = raw_data_positioning[['x_TS','y_TS','z_TS']].sort_index()\n",
    "encoder_points_01_02 = data_positioning[['x_Encoder','y_Encoder','z_Encoder']].sort_index()/1000\n",
    "data_encoder_01_02 = data_positioning[['x_Encoder', 'y_Encoder', 'z_Encoder','encoder_0','encoder_1','encoder_2','encoder_3','encoder_4','encoder_5','encoder_6','encoder_7']].sort_index()\n",
    "used_indices = set(data_positioning.index)\n",
    "available_indices = list(set(data.index) - used_indices)\n",
    "selected_indices = np.random.choice(available_indices, 20, replace=False)\n",
    "validation_data_encoder_01_02 = data.loc[selected_indices].sort_index()\n",
    "validation_data_TS_01_02 = raw_data.loc[selected_indices].sort_index()\n",
    "encoder_points_validation_01_02 = validation_data_encoder_01_02[['x_Encoder', 'y_Encoder', 'z_Encoder']].values/1000 \n",
    "ts_points_validation_01_02 = validation_data_TS_01_02[['x_TS', 'y_TS', 'z_TS']].values\n",
    "\n",
    "# Daten vom 10.02.2025 importieren und strukturieren\n",
    "data = pd.read_csv('../data/Encoderwerte_Rohdaten_10_02_25.csv', delimiter=';',index_col=0)\n",
    "ts_points_10_02 = data[['x_TS', 'y_TS', 'z_TS']].sort_index()\n",
    "encoder_points_10_02 = data[['x_Encoder', 'y_Encoder','z_Encoder']].sort_index()\n",
    "data_encoder_10_02 = data[['x_Encoder', 'y_Encoder','z_Encoder','encoder_0','encoder_1','encoder_2','encoder_3','encoder_4','encoder_5','encoder_6','encoder_7']].sort_index()\n",
    "\n",
    "# Daten vom 25.02.2025 importieren und strukturieren\n",
    "data_TS_25_02 = pd.read_csv('../data/Rohdaten_TS_25_02_E_25.csv', delimiter=';', index_col=0).sort_index()\n",
    "data_encoder_25_02 = pd.read_csv('../data/Encoderwerte_25_02_E_25.csv', delimiter=';', index_col=0).sort_index()\n",
    "ts_points_25_02 = data_TS_25_02[['x_TS', 'y_TS', 'z_TS']]\n",
    "encoder_points_25_02 = data_encoder_25_02[['x_Encoder', 'y_Encoder','z_Encoder']]/1000\n",
    "validation_data_encoder_25_02 = pd.read_csv('../data/Encoderwerte_25_02_25.csv', delimiter=';', index_col=0).sort_index()\n",
    "validation_data_TS_25_02 = pd.read_csv('../data/Rohdaten_TS_25_02_25.csv', delimiter=';', index_col=0).sort_index()\n",
    "encoder_points_validation_25_02 = validation_data_encoder_25_02[['x_Encoder', 'y_Encoder', 'z_Encoder']].values/1000\n",
    "ts_points_validation_25_02 = validation_data_TS_25_02[['x_TS', 'y_TS', 'z_TS']].values\n",
    "\n",
    "# Daten vom 03.03.2025 (Position 1) importieren und strukturieren\n",
    "data_TS_03_03 = pd.read_csv('../data/Rohdaten_TS_03_03_25_Position_1_Einmessung.csv', delimiter=';', index_col=0).sort_index()\n",
    "data_encoder_03_03 = pd.read_csv('../data/Encoderwerte_03_03_25_Position_1_Einmessung.csv', delimiter=';', index_col=0).sort_index()\n",
    "ts_points_03_03 = data_TS_03_03[['x_TS', 'y_TS', 'z_TS']]\n",
    "encoder_points_03_03 = data_encoder_03_03[['x_Encoder', 'y_Encoder','z_Encoder']]/1000\n",
    "validation_data_encoder_03_03 = pd.read_csv('../data/Encoderwerte_03_03_25_Position_1.csv', delimiter=';', index_col=0).sort_index()\n",
    "validation_data_TS_03_03 = pd.read_csv('../data/Rohdaten_TS_03_03_25_Position_1.csv', delimiter=';', index_col=0).sort_index()\n",
    "validation_data_encoder_03_03 = validation_data_encoder_03_03[~validation_data_encoder_03_03.index.str.endswith((\"second_stage_corrected\", \"first_stage_corrected\"))].sort_index()\n",
    "encoder_points_validation_03_03 = validation_data_encoder_03_03[['x_Encoder', 'y_Encoder', 'z_Encoder']].values/1000\n",
    "ts_points_validation_03_03 = validation_data_TS_03_03[['x_TS', 'y_TS', 'z_TS']].values\n",
    "\n",
    "# Daten vom 03.03.2025 (Position 2) importieren und strukturieren\n",
    "data_TS_03_03_2 = pd.read_csv('../data/Rohdaten_TS_03_03_25_Position_2_Einmessung.csv', delimiter=';', index_col=0).sort_index()\n",
    "data_encoder_03_03_2 = pd.read_csv('../data/Encoderwerte_03_03_25_Position_2_Einmessung.csv', delimiter=';', index_col=0).sort_index()\n",
    "ts_points_03_03_2 = data_TS_03_03_2[['x_TS', 'y_TS', 'z_TS']]\n",
    "encoder_points_03_03_2 = data_encoder_03_03_2[['x_Encoder', 'y_Encoder','z_Encoder']]/1000\n",
    "\n",
    "validation_data_encoder_03_03_2 = pd.read_csv('../data/Encoderwerte_03_03_25_Position_2.csv', delimiter=';', index_col=0).sort_index()\n",
    "validation_data_TS_03_03_2 = pd.read_csv('../data/Rohdaten_TS_03_03_25_Position_2.csv', delimiter=';', index_col=0).sort_index()\n",
    "validation_data_encoder_03_03_2 = validation_data_encoder_03_03_2[~validation_data_encoder_03_03_2.index.str.endswith((\"second_stage_corrected\", \"first_stage_corrected\"))].sort_index()\n",
    "encoder_points_validation_03_03_2 = validation_data_encoder_03_03_2[['x_Encoder', 'y_Encoder', 'z_Encoder']].values/1000\n",
    "ts_points_validation_03_03_2 = validation_data_TS_03_03_2[['x_TS', 'y_TS', 'z_TS']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018eff6",
   "metadata": {},
   "source": [
    "Dictionaries für alle TS-Werte, Encoder-Werte und Validierungsdaten für for-Loop erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "033cc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = {\n",
    "    \"TS_Points_21_01\": ts_points_21_01,\n",
    "    \"TS_Points_01_02\": ts_points_01_02,\n",
    "    \"TS_Points_25_02\": ts_points_25_02,\n",
    "    \"TS_Points_03_03\": ts_points_03_03,\n",
    "    \"TS_Points_03_03_2\": ts_points_03_03_2,\n",
    "}\n",
    "\n",
    "encoder_data = {\n",
    "    \"Encoder_Points_21_01\": encoder_points_21_01,\n",
    "    \"Encoder_Points_01_02\": encoder_points_01_02,\n",
    "    \"Encoder_Points_25_02\": encoder_points_25_02,\n",
    "    \"Encoder_Points_03_03\": encoder_points_03_03,\n",
    "    \"Encoder_Points_03_03_2\": encoder_points_03_03_2,\n",
    "}\n",
    "\n",
    "validation_ts_data = {\n",
    "    \"TS_Points_validation_21_01\": ts_points_validation_21_01,\n",
    "    \"TS_Points_validation_01_02\": ts_points_validation_01_02,\n",
    "    \"TS_Points_validation_25_02\": ts_points_validation_25_02,\n",
    "    \"TS_Points_validation_03_03\": ts_points_validation_03_03,\n",
    "    \"TS_Points_validation_03_03_2\": ts_points_validation_03_03_2,\n",
    "}\n",
    "\n",
    "validation_encoder_data = {\n",
    "    \"Encoder_Points_validation_21_01\": encoder_points_validation_21_01,\n",
    "    \"Encoder_Points_validation_01_02\": encoder_points_validation_01_02,\n",
    "    \"Encoder_Points_validation_25_02\": encoder_points_validation_25_02,\n",
    "    \"Encoder_Points_validation_03_03\": encoder_points_validation_03_03,\n",
    "    \"Encoder_Points_validation_03_03_2\": encoder_points_validation_03_03_2,\n",
    "}\n",
    "\n",
    "# Convert dictionary keys to lists for ordered processing\n",
    "ts_keys = list(ts_data.keys())\n",
    "enc_keys = list(encoder_data.keys())\n",
    "val_ts_keys = list(validation_ts_data.keys())\n",
    "val_enc_keys = list(validation_encoder_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ebafb",
   "metadata": {},
   "source": [
    "Auswertungskriterien festlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "795a3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_sample_size = True\n",
    "add_noise = False\n",
    "loss_function = False\n",
    "min_samples = 4\n",
    "sample_interval = 6\n",
    "noise_percent_min = 0\n",
    "noise_percent_max = 0.1\n",
    "noise_steps_number = 2\n",
    "noise_level = 0.1\n",
    "legend = False\n",
    "grid_choice = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b125da",
   "metadata": {},
   "source": [
    "Auswertungen dürchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9eb212e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(index=None)\n",
    "# Iterate while there are elements in all dictionaries\n",
    "while ts_keys and enc_keys and val_ts_keys and val_enc_keys:\n",
    "    # Get the first keys from each dictionary\n",
    "    ts_key = ts_keys.pop(0)\n",
    "    enc_key = enc_keys.pop(0)\n",
    "    val_ts_key = val_ts_keys.pop(0)\n",
    "    val_enc_key = val_enc_keys.pop(0)\n",
    "    \n",
    "    # Extract the first series from each dictionary\n",
    "    ts_points = ts_data.pop(ts_key)  # Remove and get the time series data\n",
    "    encoder_points = encoder_data.pop(enc_key)  # Remove and get the encoder data\n",
    "    ts_points_validation = validation_ts_data.pop(val_ts_key)  # Remove and get the validation data\n",
    "    encoder_points_validation = validation_encoder_data.pop(val_enc_key)  # Remove and get the validation data\n",
    "    \n",
    "    if evaluate_sample_size:\n",
    "        for num_points_data in range(min_samples, len(ts_points), sample_interval):\n",
    "            # Zufällige Punkte auswählen\n",
    "            indices = np.random.choice(ts_points.index, num_points_data, replace=False)\n",
    "\n",
    "            random_ts_points = ts_points.loc[indices]\n",
    "            random_encoder_points = encoder_points.loc[indices]\n",
    "\n",
    "            df_result.loc[len(df_result),'sample_size'] = num_points_data\n",
    "                    # Ensure they have the same label\n",
    "            assert all(random_ts_points.index == random_encoder_points.index), \"Indices do not match!\"\n",
    "\n",
    "            P = random_ts_points.values\n",
    "            Q = random_encoder_points.values\n",
    "            if add_noise:\n",
    "                percentages = np.linspace(noise_percent_min, noise_percent_max, noise_steps_number)\n",
    "            else:\n",
    "                percentages = [0]\n",
    "            for percent in percentages:\n",
    "                if percent > 0:\n",
    "                    P_noisy = add_noise_to_random_points(P, noise_level, num_points= math.ceil(num_points_data*percent))\n",
    "\n",
    "                    #Check noise\n",
    "                    noise_check = np.linalg.norm(P_noisy - P, axis=1)*1000\n",
    "                    P = P_noisy\n",
    "                \n",
    "                # ------------------------------\n",
    "                # Erster Schritt: Kabsch-Algorithmus zur Initialschätzung\n",
    "                # ------------------------------\n",
    "                R_est, t_est = kabsch_algorithm(P, Q)\n",
    "\n",
    "                # Wir gehen davon aus, dass R_est bereits sehr nahe ist. \n",
    "                # Für die robuste Optimierung parametrisieren wir R über Eulerwinkel.\n",
    "                # Hier vereinfachen wir und starten mit Eulerwinkeln nahe Null.\n",
    "                x0 = np.array([0.0, 0.0, 0.0, t_est[0], t_est[1], t_est[2]])\n",
    "                \n",
    "                if loss_function:\n",
    "                    methods = ['huber', 'soft_l1', 'cauchy', 'arctan', 'linear']\n",
    "                else:\n",
    "                    methods = ['huber']\n",
    "                for method_loss in methods:\n",
    "                    # ------------------------------\n",
    "                    # Robuste Optimierung mit Huber-Loss\n",
    "                    # ------------------------------\n",
    "                    result = least_squares(residual, x0, args=(P, Q), loss= method_loss, f_scale=1.0)\n",
    "                    alpha_opt, beta_opt, gamma_opt, tx_opt, ty_opt, tz_opt = result.x\n",
    "                    residuals_after_kabsch_local = residual(result.x, P, Q)\n",
    "                    R_opt = rotation_matrix_from_euler(alpha_opt, beta_opt, gamma_opt)\n",
    "                    t_opt = np.array([tx_opt, ty_opt, tz_opt])\n",
    "\n",
    "                    # Berechnen Sie den durchschnittlichen Fehler nach der Optimierung\n",
    "                    err_final = np.linalg.norm((R_opt @ P.T).T + t_opt - Q, axis=1)\n",
    "                    err_final_mean = np.mean(err_final)\n",
    "                    date_key = ts_key.replace('TS_Points_', '')\n",
    "                    # Properly append the results as a new row\n",
    "                    label_col = 'error_validation_' + str(percent)  + '_' + method_loss + '_' + date_key\n",
    "                    # ------------------------------\n",
    "                    # Validation\n",
    "                    # ------------------------------\n",
    "                    ts_points_validation_transformed = (R_opt @ ts_points_validation.T).T + t_opt\n",
    "\n",
    "                    err_final_validation = np.linalg.norm(ts_points_validation_transformed - encoder_points_validation, axis=1)\n",
    "                    df_result.loc[len(df_result)-1, label_col] = np.mean(err_final_validation)*1000\n",
    "\n",
    "for col in df_result.columns:\n",
    "    if col == 'sample_size':\n",
    "        continue\n",
    "    date_dataset = col.split('_')[-3] + '_' + col.split('_')[-2] + '_' + col.split('_')[-1]\n",
    "    date_key = '_' + date_dataset.replace('huber_', '')\n",
    "    plt.scatter(df_result['sample_size'],df_result[col], label= col, marker='x')\n",
    "    plt.xlabel('Anzahl der Stichproben')\n",
    "    plt.ylabel('Mittlerer Fehler der Norm in mm')\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.savefig('../results/Diagramme/Einmessverfahren/Einfluss_Stichprobenanzahl_Einmessung' + date_key + '.pdf')\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
